# ğŸ“Œ Project Title: AlgoSage â€“ AI-Powered Code Reviewer  

## ğŸš€ Project Overview  
**AlgoSage** is an AI-powered assistant that reviews source code, detects potential bugs, and suggests performance improvements. It leverages Large Language Models (LLMs) with prompt engineering techniques to analyze code and provide structured, developer-friendly feedback.  

The project explores advanced prompting methods such as zero-shot, one-shot, multi-shot, dynamic prompting, and chain-of-thought prompting to enhance review accuracy. Additionally, it integrates embeddings and vector databases to retrieve relevant past bug fixes, making recommendations context-aware.  

## ğŸ”§ Features  
- **Error Detection & Suggestions** â€“ Identifies syntax, logical, and optimization issues.  
- **Structured Output** â€“ Returns feedback in JSON for easy integration.  
- **Prompt Engineering** â€“ Implements zero-shot, one-shot, multi-shot, dynamic, and chain-of-thought prompting.  
- **Evaluation Pipeline** â€“ Uses a dataset of code snippets to benchmark outputs against expected fixes.  
- **Similarity Search** â€“ Embeddings + vector database to match bugs with prior solutions.  
- **Function Calling** â€“ `analyzeCode(language, snippet)` to automate code review.  

## ğŸ¯ Tech Stack  
- **Backend**: Node.js / Python  
- **LLM**: OpenAI / Hugging Face API  
- **Database**: Vector DB (Pinecone / FAISS)  
- **Evaluation**: Custom testing framework with similarity metrics (Cosine, L2, Dot Product).  

## ğŸ“ˆ Future Scope  
- Multi-language support  
- Integration with GitHub/GitLab CI/CD for automated pull request reviews  

## ğŸ“ System and User Prompts  

To implement AlgoSage, we define **clear prompts** for the AI, ensuring consistent and accurate reviews.  
These prompts are designed using the **RTFC Framework**.  

### ğŸ”¹ System Prompt  
You are an AI Code Reviewer. Your role is to analyze the given code, identify bugs, suggest improvements, and provide structured feedback. Always return results in a JSON format containing:  
- `issues`: List of detected bugs or problems  
- `suggestions`: Recommended improvements  
- `overall_feedback`: Summary of code quality  

### ğŸ”¹ User Prompt  
Review the following Python code and provide feedback as per the defined schema:  

```python
def add_numbers(a, b):
    return a - b  # intended to be addition
````

### ğŸ“Œ RTFC Framework Usage

* **R (Role):** Defined in the system prompt as a code reviewer.
* **T (Task):** Analyze code, detect bugs, and suggest improvements.
* **F (Format):** Responses must follow a structured JSON output.
* **C (Context):** The provided code snippet and programming language.

## ğŸ¯ Zero-Shot Prompting  

In AlgoSage, we apply **Zero-Shot Prompting**, where the AI is asked to perform the task without any prior examples.  
Instead of showing sample inputs/outputs, the AI directly relies on the instructions to understand the task.  
This makes the system **flexible, adaptive, and language-agnostic**.  

### ğŸ”¹ Zero-Shot Prompt  

**System Prompt:**  
You are an AI code reviewer. Analyze the given code, detect bugs, and suggest improvements.  
Always return results in JSON format with three fields: `issues`, `suggestions`, and `overall_feedback`.  

**User Prompt:**  
Review the following JavaScript code and provide structured feedback:  

```javascript
function addition(a, b) {
    return a + b; // intended to be addition 
}
````

### ğŸ“Œ Why Zero-Shot Prompting?

* The AI is not given examples â€” only task-specific instructions.
* It can generalize across **multiple programming languages** and scenarios.
* Ensures **scalable code reviews** without needing predefined cases.



## ğŸ¯ One-Shot Prompting  

In AlgoSage, we use **One-Shot Prompting**, where the AI is given a **single example** before performing the task.  
This helps guide the model with a reference while still keeping it generalizable.  

### ğŸ”¹ One-Shot Prompt  

**System Prompt:**  
You are an AI code reviewer. Analyze the given code, detect bugs, and suggest improvements.  
Always return results in JSON format with three fields: `issues`, `suggestions`, and `overall_feedback`.  

**User Prompt (with one example):**  

Example Input (Python):  
```python
def divide(a, b):
    return a * b  # intended to be division
````

Example Expected Output (JSON):

```json
{
  "issues": ["The operator used is multiplication instead of division."],
  "suggestions": ["Replace '*' with '/' to correctly divide the numbers."],
  "overall_feedback": "Logic error detected in the function implementation."
}
```

Now review the following Java code:

```java
public int subtract(int a, int b) {
    return a + b; // intended to be subtraction
}
```

### ğŸ“Œ Why One-Shot Prompting?

* Provides **one guiding example** to set the response pattern.
* Ensures the AI generates **consistent, structured outputs**.
* Reduces ambiguity compared to zero-shot prompting.


## ğŸ¯ Multi-Shot Prompting  

In AlgoSage, we apply **Multi-Shot Prompting**, where the AI is provided with **multiple examples** before being asked to solve the real task.  
This method ensures the AI clearly understands the **pattern, structure, and expectations** of the output.  

### ğŸ”¹ Multi-Shot Prompt  

**System Prompt:**  
You are an AI code reviewer. Analyze the given code, detect bugs, and suggest improvements.  
Always return results in JSON format with three fields: `issues`, `suggestions`, and `overall_feedback`.  

**User Prompt (with multiple examples):**  

Example 1 (Python):  
```python
def add(a, b):
    return a - b  # intended to be addition
````

Expected Output:

```json
{
  "issues": ["The operator used is subtraction instead of addition."],
  "suggestions": ["Replace '-' with '+' to correctly add the numbers."],
  "overall_feedback": "Logic error detected in the addition function."
}
```

Example 2 (JavaScript):

```javascript
function square(n) {
    return n + n; // intended to be square
}
```

Expected Output:

```json
{
  "issues": ["The function doubles the number instead of squaring it."],
  "suggestions": ["Replace 'n + n' with 'n * n' to correctly square the number."],
  "overall_feedback": "Incorrect mathematical operation for squaring."
}
```

Now review the following C++ code:

```cpp
int multiply(int a, int b) {
    return a - b; // intended to be multiplication
}
```

###  Why Multi-Shot Prompting?

* Gives the AI **multiple reference patterns** to ensure consistent results.
* Helps in **complex tasks** where one example isnâ€™t enough.
* Reduces errors and improves **accuracy of code reviews**.

##  Dynamic Prompting  

In AlgoSage, we use **Dynamic Prompting**, where the prompt is automatically adapted based on the **userâ€™s input context** (e.g., programming language, code style, or desired output format).  
This makes the system **flexible and personalized**, instead of relying on fixed instructions.  

### ğŸ”¹ Dynamic Prompt Example  

**System Prompt (Template):**  
You are an AI code reviewer. Analyze the given code in **{{language}}**, detect bugs, and suggest improvements.  
Always return results in JSON format with three fields: `issues`, `suggestions`, and `overall_feedback`.  

**User Prompt (Generated Dynamically):**  
Review the following **{{language}}** code:  

```{{language}}
{{code_snippet}}
````

### Example Execution

If the user provides **Python code**:

```python
def divide(a, b):
    return a + b  # intended to be division
```

The dynamically generated prompt becomes:

```text
You are an AI code reviewer. Analyze the given code in Python, detect bugs, and suggest improvements.
Always return results in JSON format with three fields: issues, suggestions, and overall_feedback.

Review the following Python code:
def divide(a, b):
    return a * b  # intended to be division
```

Expected Output:

```json
{
  "issues": ["The function multiplies instead of dividing."],
  "suggestions": ["Replace '*' with '/' to correctly perform division."],
  "overall_feedback": "Incorrect operator used for division."
}
```

###  Why Dynamic Prompting?

* Automatically adapts prompts to **any programming language**.
* Makes the system more **scalable and user-specific**.
* Reduces manual work while ensuring **consistent structured outputs**.


## ğŸ¯ Chain-of-Thought Prompting  

In algoSage, we apply **Chain-of-Thought (CoT) Prompting**, where the AI is encouraged to **reason step by step** before providing the final answer.  
This helps the system explain *why* a piece of code is incorrect, not just point out the issue.  

### ğŸ”¹ Chain-of-Thought Prompt  

**System Prompt:**  
You are an AI code reviewer. Analyze the given code step by step (reasoning internally), then provide your final answer only in JSON format with three fields: `issues`, `suggestions`, and `overall_feedback`. Do not reveal the reasoning steps to the user.  

**User Prompt:**  
Review the following Python code:  

```python
def is_even(n):
    if n % 2 == 1:
        return True
    return False
````

### ğŸ”¹ Expected Reasoning (hidden to user):

1. The function is meant to check if a number is even.
2. Current condition checks `n % 2 == 1`, which actually detects **odd numbers**.
3. The return values are inverted.
4. Correct logic should be `n % 2 == 0`.

### ğŸ”¹ Final Output (shown to user):

```json
{
  "issues": ["The condition checks for odd numbers instead of even."],
  "suggestions": ["Change condition to 'n % 2 == 0' for even check."],
  "overall_feedback": "The function incorrectly identifies odd numbers as even."
}
```

### ğŸ“Œ Why Chain-of-Thought Prompting?

* Ensures **deeper reasoning** behind feedback.
* Reduces **false positives/negatives** in bug detection.
* Provides **explainable AI** code reviews with accurate fixes.



## ğŸ§ª Evaluation Dataset & Testing Framework  

To ensure **AlgoSage** works reliably, we created an **evaluation pipeline** with:  
- A dataset of **5+ sample code snippets**  
- A **judge prompt** to compare AI output with expected results  
- A lightweight **testing framework** to run all cases  

---

### ğŸ“‚ Evaluation Dataset (5 Samples)  

```json
[
  {
    "id": 1,
    "language": "Python",
    "code": "def add(a, b): return a - b",
    "expected": "Bug: subtraction instead of addition"
  },
  {
    "id": 2,
    "language": "JavaScript",
    "code": "function isPositive(n) { return n < 0; }",
    "expected": "Bug: returns true for negative numbers"
  },
  {
    "id": 3,
    "language": "Java",
    "code": "boolean isEven(int n) { return n % 2 == 1; }",
    "expected": "Bug: detects odd numbers instead of even"
  },
  {
    "id": 4,
    "language": "C++",
    "code": "int divide(int a, int b) { return a / b; }",
    "expected": "Edge Case: No handling of division by zero"
  },
  {
    "id": 5,
    "language": "Python",
    "code": "def factorial(n): return n * factorial(n-1)",
    "expected": "Bug: No base case, leads to infinite recursion"
  }
]
````

---

### ğŸ§‘â€âš–ï¸ Judge Prompt

```text
You are a judge. Compare the model's output with the expected result.  
Evaluation Parameters:  
1. **Correctness** â€“ Did the model identify the main bug?  
2. **Completeness** â€“ Did it also suggest a meaningful fix?  
3. **Format** â€“ Is the response in the required JSON structure?  
4. **Clarity** â€“ Is the feedback understandable?  

Return: Pass / Fail with justification.
```

---

### âš™ï¸ Testing Framework Setup

We built a **simple test runner** that:

1. Iterates over all dataset samples.
2. Sends each code snippet to the model.
3. Captures the AI output and passes it to the **judge prompt**.
4. Logs results as âœ… Pass / âŒ Fail with explanations.

```python
for test in dataset:
    ai_output = run_model(test["code"])
    verdict = judge(ai_output, test["expected"])
    print(f"Test {test['id']}: {verdict}")
```

---

### âœ… Why This Setup?
* Ensures **objective evaluation** of AI outputs.
* Provides **clear metrics** on accuracy and reliability.
* Makes the pipeline **reproducible** for future improvements.